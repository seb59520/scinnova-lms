# Databricks notebook source
# MAGIC %md
# MAGIC # üî¨ Notebook 2 ‚Äî DATA SCIENCE : Qualit√©, EDA, Features
# MAGIC 
# MAGIC ## üéØ Objectif
# MAGIC **Rendre la donn√©e fiable et construire une cible ML pertinente**
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC ### Ce que vous allez apprendre
# MAGIC 1. Contr√¥ler la qualit√© des donn√©es (nulls, aberrations)
# MAGIC 2. Explorer les distributions et corr√©lations (EDA)
# MAGIC 3. Cr√©er des features pertinentes pour le ML
# MAGIC 4. D√©finir une cible m√©tier compr√©hensible
# MAGIC 
# MAGIC ### Dur√©e estim√©e : 45-60 minutes
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC ## üìä Contexte M√©tier
# MAGIC 
# MAGIC L'√©quipe Marketing vous demande d'identifier les **commandes √† forte valeur** pour :
# MAGIC - Prioriser le service client premium
# MAGIC - D√©clencher des offres de fid√©lisation
# MAGIC - Anticiper la charge logistique
# MAGIC 
# MAGIC Vous devez pr√©parer un jeu de donn√©es pour entra√Æner un mod√®le de classification.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1Ô∏è‚É£ Chargement des Donn√©es depuis Delta

# COMMAND ----------

# Chargement de la table Delta cr√©√©e dans le Notebook 1
sales = spark.table("sales_delta")

print(f"üìä Nombre de lignes : {sales.count():,}")
sales.printSchema()

# COMMAND ----------

display(sales.limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2Ô∏è‚É£ Contr√¥le Qualit√© des Donn√©es
# MAGIC 
# MAGIC ### Objectif : d√©tecter les probl√®mes avant qu'ils ne biaisent nos analyses
# MAGIC 
# MAGIC Checklist qualit√© :
# MAGIC - ‚úÖ Valeurs nulles
# MAGIC - ‚úÖ Valeurs aberrantes (outliers)
# MAGIC - ‚úÖ Coh√©rence des domaines (valeurs attendues)
# MAGIC - ‚úÖ Doublons √©ventuels

# COMMAND ----------

# MAGIC %md
# MAGIC ### üîç 2.1 D√©tection des Valeurs Nulles

# COMMAND ----------

from pyspark.sql.functions import col, when, isnan, sum as _sum, count

# Comptage des nulls et NaN par colonne
null_counts = sales.select([
    _sum(
        when(col(c).isNull() | (col(c) == "") | isnan(col(c)), 1).otherwise(0)
    ).alias(c)
    for c in sales.columns
])

display(null_counts)

# COMMAND ----------

# Version en pourcentage
total_rows = sales.count()

null_pct = sales.select([
    (
        _sum(when(col(c).isNull() | (col(c) == ""), 1).otherwise(0)) / total_rows * 100
    ).alias(c)
    for c in sales.columns
])

print("üìä Pourcentage de valeurs nulles par colonne :")
display(null_pct)

# COMMAND ----------

# MAGIC %md
# MAGIC ### ‚úÖ R√©sultat Qualit√© Nulls
# MAGIC 
# MAGIC Notre dataset simul√© est propre (0% de nulls). En production, vous auriez probablement :
# MAGIC - Des dates manquantes
# MAGIC - Des pays non renseign√©s
# MAGIC - Des prix √† 0 ou n√©gatifs
# MAGIC 
# MAGIC **Actions typiques** : imputation, suppression, signalement

# COMMAND ----------

# MAGIC %md
# MAGIC ### üîç 2.2 D√©tection des Valeurs Aberrantes

# COMMAND ----------

from pyspark.sql.functions import min as _min, max as _max, avg, stddev

# Statistiques sur les num√©riques
stats_num = sales.select(
    _min("price").alias("price_min"),
    _max("price").alias("price_max"),
    avg("price").alias("price_avg"),
    stddev("price").alias("price_std"),
    _min("quantity").alias("qty_min"),
    _max("quantity").alias("qty_max"),
    avg("quantity").alias("qty_avg"),
    _min("revenue").alias("rev_min"),
    _max("revenue").alias("rev_max"),
    avg("revenue").alias("rev_avg"),
)

display(stats_num)

# COMMAND ----------

# V√©rification : y a-t-il des prix n√©gatifs ou nuls ?
prix_invalides = sales.filter((col("price") <= 0) | (col("quantity") <= 0))
print(f"‚ö†Ô∏è Lignes avec prix/quantit√© invalide : {prix_invalides.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### üîç 2.3 V√©rification des Domaines Cat√©goriels

# COMMAND ----------

# Valeurs uniques par colonne cat√©gorielle
cat_cols = ["product", "category", "country", "channel", "payment"]

for col_name in cat_cols:
    distinct_values = sales.select(col_name).distinct().count()
    print(f"üìã {col_name}: {distinct_values} valeurs uniques")

# COMMAND ----------

# D√©tail des valeurs par dimension
for col_name in cat_cols:
    print(f"\nüìã Valeurs de '{col_name}':")
    display(sales.groupBy(col_name).count().orderBy(col("count").desc()))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3Ô∏è‚É£ Analyse Exploratoire (EDA)
# MAGIC 
# MAGIC ### Objectif : comprendre les distributions et identifier des patterns

# COMMAND ----------

# MAGIC %md
# MAGIC ### üìä 3.1 Distribution du Revenue

# COMMAND ----------

# Statistiques descriptives du revenue
display(sales.select("revenue").describe())

# COMMAND ----------

# Distribution par tranches de revenue
from pyspark.sql.functions import when, lit

tranches = (
    sales
    .withColumn("tranche_revenue", 
        when(col("revenue") < 100, "< 100‚Ç¨")
        .when(col("revenue") < 500, "100-500‚Ç¨")
        .when(col("revenue") < 1000, "500-1000‚Ç¨")
        .when(col("revenue") < 2000, "1000-2000‚Ç¨")
        .otherwise("> 2000‚Ç¨")
    )
    .groupBy("tranche_revenue")
    .count()
    .orderBy("tranche_revenue")
)

display(tranches)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üìä 3.2 Panier Moyen par Dimension

# COMMAND ----------

from pyspark.sql.functions import round as spark_round

# Panier moyen par pays
panier_pays = (
    sales
    .groupBy("country")
    .agg(
        spark_round(avg("revenue"), 2).alias("panier_moyen"),
        count("*").alias("nb_commandes")
    )
    .orderBy(col("panier_moyen").desc())
)

display(panier_pays)

# COMMAND ----------

# Panier moyen par canal
panier_canal = (
    sales
    .groupBy("channel")
    .agg(
        spark_round(avg("revenue"), 2).alias("panier_moyen"),
        count("*").alias("nb_commandes")
    )
    .orderBy(col("panier_moyen").desc())
)

display(panier_canal)

# COMMAND ----------

# Panier moyen par cat√©gorie de produit
panier_categorie = (
    sales
    .groupBy("category")
    .agg(
        spark_round(avg("revenue"), 2).alias("panier_moyen"),
        count("*").alias("nb_commandes")
    )
    .orderBy(col("panier_moyen").desc())
)

display(panier_categorie)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üìä 3.3 Analyse Temporelle

# COMMAND ----------

from pyspark.sql.functions import month, dayofweek, year

# CA par mois
ca_mensuel = (
    sales
    .withColumn("mois", month(col("order_date")))
    .groupBy("mois")
    .agg(_sum("revenue").alias("ca_total"))
    .orderBy("mois")
)

display(ca_mensuel)

# COMMAND ----------

# CA par jour de la semaine (1=Dimanche, 7=Samedi)
ca_jour_semaine = (
    sales
    .withColumn("jour_semaine", dayofweek(col("order_date")))
    .groupBy("jour_semaine")
    .agg(
        _sum("revenue").alias("ca_total"),
        count("*").alias("nb_commandes")
    )
    .orderBy("jour_semaine")
)

display(ca_jour_semaine)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üí° Insights EDA
# MAGIC 
# MAGIC Observations cl√©s √† noter :
# MAGIC 1. **Saisonnalit√©** : pics en novembre/d√©cembre (Black Friday, No√´l)
# MAGIC 2. **Canal Web** : panier moyen plus √©lev√© que Mobile
# MAGIC 3. **Informatique** : cat√©gorie avec le CA moyen le plus fort
# MAGIC 4. **France** : march√© dominant en volume

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4Ô∏è‚É£ Cr√©ation de la Cible ML
# MAGIC 
# MAGIC ### Probl√©matique M√©tier
# MAGIC 
# MAGIC > "Comment identifier √† l'avance les commandes √† forte valeur ?"
# MAGIC 
# MAGIC ### Choix : Classification Binaire
# MAGIC 
# MAGIC Nous cr√©ons une cible **`high_value_order`** :
# MAGIC - `1` si la commande est dans le **top 20%** des revenues
# MAGIC - `0` sinon
# MAGIC 
# MAGIC Cette approche est :
# MAGIC - ‚úÖ Simple √† expliquer m√©tier
# MAGIC - ‚úÖ √âquilibr√©e (80/20)
# MAGIC - ‚úÖ Actionnable (priorisation client)

# COMMAND ----------

# Calcul du seuil : percentile 80
seuil_p80 = sales.approxQuantile("revenue", [0.80], 0.01)[0]
print(f"üí∞ Seuil top 20% : {seuil_p80:.2f} ‚Ç¨")

# COMMAND ----------

# Cr√©ation de la cible + features temporelles
ds = (
    sales
    # Cible binaire
    .withColumn("high_value_order", 
                when(col("revenue") >= seuil_p80, 1).otherwise(0).cast("int"))
    # Features temporelles
    .withColumn("month", month(col("order_date")))
    .withColumn("dow", dayofweek(col("order_date")))  # Day of Week
    .withColumn("year", year(col("order_date")))
)

# V√©rification de la distribution de la cible
display(
    ds
    .groupBy("high_value_order")
    .agg(
        count("*").alias("nb_commandes"),
        spark_round(avg("revenue"), 2).alias("revenue_moyen")
    )
)

# COMMAND ----------

# MAGIC %md
# MAGIC ### ‚úÖ √âquilibre de la Cible
# MAGIC 
# MAGIC - **~80%** des commandes sont class√©es `0` (valeur normale)
# MAGIC - **~20%** des commandes sont class√©es `1` (haute valeur)
# MAGIC 
# MAGIC C'est un d√©s√©quilibre mod√©r√©, acceptable pour un premier mod√®le.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5Ô∏è‚É£ Pr√©paration du Dataset ML
# MAGIC 
# MAGIC ### S√©lection des Features
# MAGIC 
# MAGIC | Type | Colonnes |
# MAGIC |------|----------|
# MAGIC | **Num√©riques** | `price`, `quantity`, `month`, `dow` |
# MAGIC | **Cat√©gorielles** | `country`, `channel`, `payment`, `category`, `product` |
# MAGIC | **Cible** | `high_value_order` |
# MAGIC 
# MAGIC ‚ö†Ô∏è On **exclut** `revenue` car c'est la variable utilis√©e pour cr√©er la cible (fuite de donn√©es)

# COMMAND ----------

# S√©lection des colonnes pour le ML
ml_df = ds.select(
    # Features num√©riques
    "price", "quantity", "month", "dow",
    # Features cat√©gorielles
    "country", "channel", "payment", "category", "product",
    # Cible
    "high_value_order"
)

print(f"üìä Dataset ML : {ml_df.count():,} lignes, {len(ml_df.columns)} colonnes")
ml_df.printSchema()

# COMMAND ----------

display(ml_df.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6Ô∏è‚É£ Sauvegarde du Dataset ML

# COMMAND ----------

# Sauvegarde en table Delta pour le Notebook 3
ml_df.write.mode("overwrite").format("delta").saveAsTable("sales_ml_ready")

print("‚úÖ Table 'sales_ml_ready' cr√©√©e avec succ√®s !")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Synth√®se Notebook 2
# MAGIC 
# MAGIC ### Ce que nous avons accompli
# MAGIC 
# MAGIC | √âtape | R√©sultat |
# MAGIC |-------|----------|
# MAGIC | Qualit√© | 0 null, pas d'aberration, domaines coh√©rents |
# MAGIC | EDA | Patterns identifi√©s (saisonnalit√©, canaux, pays) |
# MAGIC | Cible ML | `high_value_order` binaire (top 20%) |
# MAGIC | Features | 9 colonnes (4 num + 5 cat) |
# MAGIC 
# MAGIC ### Messages cl√©s
# MAGIC 
# MAGIC 1. **Data Quality** = fondation de tout projet data
# MAGIC 2. **EDA** = comprendre avant de mod√©liser
# MAGIC 3. **Feature Engineering** = transformer la connaissance m√©tier en variables
# MAGIC 
# MAGIC ### Prochaine √©tape
# MAGIC 
# MAGIC üëâ **Notebook 3 : Machine Learning** ‚Äî Entra√Æner un mod√®le de classification

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## üìù Exercices Pratiques
# MAGIC 
# MAGIC ### Exercice 1 : Feature "is_weekend"
# MAGIC Cr√©ez une colonne binaire `is_weekend` (1 si samedi/dimanche, 0 sinon).
# MAGIC 
# MAGIC ### Exercice 2 : Analyse Crois√©e
# MAGIC Quel canal a le plus fort taux de commandes "high value" par pays ?
# MAGIC 
# MAGIC ### Exercice 3 : Seuil Alternatif
# MAGIC Que se passe-t-il si on utilise le percentile 90 au lieu de 80 ?

# COMMAND ----------

# üéØ EXERCICE 1 : Votre code ici
# Indice : dow == 1 (dimanche) ou dow == 7 (samedi)

# ds_weekend = ds.withColumn("is_weekend", 
#     when((col("dow") == 1) | (col("dow") == 7), 1).otherwise(0)
# )

# COMMAND ----------

# üéØ EXERCICE 2 : Votre code ici
# Indice : groupBy("country", "channel"), calculer le taux de high_value_order



# COMMAND ----------

# üéØ EXERCICE 3 : Votre code ici
# Indice : recalculer avec approxQuantile(..., [0.90], ...)


