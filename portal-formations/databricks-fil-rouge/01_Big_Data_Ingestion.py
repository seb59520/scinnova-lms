# Databricks notebook source
# MAGIC %md
# MAGIC # üì¶ Notebook 1 ‚Äî BIG DATA : Ingestion, Delta, Agr√©gations
# MAGIC 
# MAGIC ## üéØ Objectif
# MAGIC **Passer du CSV brut √† une table exploitable, optimis√©e pour l'analyse**
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC ### Ce que vous allez apprendre
# MAGIC 1. Charger un fichier CSV massif avec Spark
# MAGIC 2. Normaliser les types de donn√©es
# MAGIC 3. Persister en format Delta Lake (performant)
# MAGIC 4. R√©aliser des analyses BI distribu√©es
# MAGIC 
# MAGIC ### Dur√©e estim√©e : 45-60 minutes
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC ## üìä Contexte M√©tier
# MAGIC 
# MAGIC Vous √™tes Data Engineer chez un e-commer√ßant europ√©en. L'√©quipe commerciale vous demande :
# MAGIC - Un tableau de bord des ventes par pays
# MAGIC - Le classement des produits best-sellers
# MAGIC - L'√©volution du chiffre d'affaires dans le temps
# MAGIC 
# MAGIC Vous disposez d'un export de **2 millions de commandes** sur 2 ans.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1Ô∏è‚É£ Configuration & Lecture du CSV
# MAGIC 
# MAGIC ### Pourquoi Spark ?
# MAGIC 
# MAGIC | Outil | Limite pratique | Temps 2M lignes |
# MAGIC |-------|-----------------|-----------------|
# MAGIC | Excel | ~1M lignes | ‚ùå Impossible |
# MAGIC | Pandas | ~10M lignes (RAM) | ~30 secondes |
# MAGIC | **Spark** | **Illimit√© (distribu√©)** | **~5 secondes** |
# MAGIC 
# MAGIC Spark distribue le traitement sur plusieurs machines (workers), permettant de scaler horizontalement.

# COMMAND ----------

# üìÅ Chemin vers le fichier CSV upload√©
# ‚ö†Ô∏è ADAPTER CE CHEMIN selon votre upload
PATH = "dbfs:/FileStore/tables/sales_2M.csv"

# COMMAND ----------

# Lecture du CSV avec inf√©rence de sch√©ma
df_raw = (
    spark.read
    .option("header", True)      # La premi√®re ligne contient les noms de colonnes
    .option("inferSchema", True) # Spark devine les types automatiquement
    .csv(PATH)
)

# Affichage du sch√©ma inf√©r√©
print("üìã Sch√©ma inf√©r√© par Spark :")
df_raw.printSchema()

# COMMAND ----------

# Aper√ßu des premi√®res lignes
display(df_raw.limit(10))

# COMMAND ----------

# Nombre total de lignes
nb_lignes = df_raw.count()
print(f"üìä Nombre total de commandes : {nb_lignes:,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### üîç Observations
# MAGIC 
# MAGIC - `order_id` : identifiant unique de commande
# MAGIC - `order_date` : date au format string (√† convertir)
# MAGIC - `product`, `category`, `country`, `channel`, `payment` : dimensions cat√©gorielles
# MAGIC - `price`, `quantity` : mesures num√©riques
# MAGIC 
# MAGIC **‚ö†Ô∏è Probl√®me** : `inferSchema` n'est pas toujours fiable. V√©rifions et normalisons les types.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2Ô∏è‚É£ Normalisation des Types de Donn√©es
# MAGIC 
# MAGIC ### Bonnes pratiques
# MAGIC 1. Toujours **caster explicitement** les colonnes critiques
# MAGIC 2. Cr√©er les **colonnes calcul√©es** n√©cessaires (`revenue`)
# MAGIC 3. G√©rer les **valeurs nulles** potentielles

# COMMAND ----------

from pyspark.sql.functions import col, to_date, round as spark_round

# Normalisation des types + calcul du revenue
df = (
    df_raw
    # Conversion de la date
    .withColumn("order_date", to_date(col("order_date"), "yyyy-MM-dd"))
    # Typage explicite des num√©riques
    .withColumn("price", col("price").cast("double"))
    .withColumn("quantity", col("quantity").cast("int"))
    # Calcul du chiffre d'affaires par ligne
    .withColumn("revenue", spark_round(col("price") * col("quantity"), 2))
)

# V√©rification du nouveau sch√©ma
print("‚úÖ Sch√©ma normalis√© :")
df.printSchema()

# COMMAND ----------

# Aper√ßu avec la nouvelle colonne revenue
display(df.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3Ô∏è‚É£ Persistance en Delta Lake
# MAGIC 
# MAGIC ### Pourquoi Delta Lake ?
# MAGIC 
# MAGIC | Caract√©ristique | CSV | Parquet | **Delta** |
# MAGIC |-----------------|-----|---------|-----------|
# MAGIC | Compression | ‚ùå | ‚úÖ | ‚úÖ |
# MAGIC | Sch√©ma enforc√© | ‚ùå | ‚úÖ | ‚úÖ |
# MAGIC | Transactions ACID | ‚ùå | ‚ùå | ‚úÖ |
# MAGIC | Time Travel | ‚ùå | ‚ùå | ‚úÖ |
# MAGIC | Updates/Deletes | ‚ùå | ‚ùå | ‚úÖ |
# MAGIC 
# MAGIC **Delta Lake** = Parquet + transaction log + versioning

# COMMAND ----------

# √âcriture en table Delta
# mode("overwrite") : remplace si existe d√©j√†
df.write.mode("overwrite").format("delta").saveAsTable("sales_delta")

print("‚úÖ Table 'sales_delta' cr√©√©e avec succ√®s !")

# COMMAND ----------

# V√©rification : lecture depuis la table Delta
sales = spark.table("sales_delta")
print(f"üìä Lignes dans sales_delta : {sales.count():,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### üïê Time Travel (bonus)
# MAGIC 
# MAGIC Delta Lake conserve l'historique des versions. Utile pour :
# MAGIC - Auditer les modifications
# MAGIC - Revenir en arri√®re en cas d'erreur
# MAGIC - Reproduire des analyses pass√©es

# COMMAND ----------

# Historique de la table
display(spark.sql("DESCRIBE HISTORY sales_delta"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4Ô∏è‚É£ Analyses BI Distribu√©es
# MAGIC 
# MAGIC Maintenant que les donn√©es sont propres et optimis√©es, r√©pondons aux questions m√©tier.

# COMMAND ----------

# MAGIC %md
# MAGIC ### üìà Analyse 1 : Chiffre d'Affaires par Pays

# COMMAND ----------

from pyspark.sql.functions import sum as _sum, format_number

# CA par pays, tri√© d√©croissant
ca_pays = (
    sales
    .groupBy("country")
    .agg(
        _sum("revenue").alias("ca_total"),
        _sum("quantity").alias("nb_articles")
    )
    .withColumn("ca_formatted", format_number("ca_total", 0))
    .orderBy(col("ca_total").desc())
)

display(ca_pays)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üèÜ Analyse 2 : Top 10 Produits (Best-sellers)

# COMMAND ----------

# Top produits par CA
top_produits = (
    sales
    .groupBy("product", "category")
    .agg(
        _sum("revenue").alias("ca_total"),
        _sum("quantity").alias("nb_vendus")
    )
    .orderBy(col("ca_total").desc())
    .limit(10)
)

display(top_produits)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üìÖ Analyse 3 : √âvolution du CA Journalier

# COMMAND ----------

# CA par jour
ca_journalier = (
    sales
    .groupBy("order_date")
    .agg(_sum("revenue").alias("ca"))
    .orderBy("order_date")
)

display(ca_journalier)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üõí Analyse 4 : Performance par Canal de Vente

# COMMAND ----------

from pyspark.sql.functions import avg, count

# Stats par canal
stats_canal = (
    sales
    .groupBy("channel")
    .agg(
        count("*").alias("nb_commandes"),
        _sum("revenue").alias("ca_total"),
        avg("revenue").alias("panier_moyen")
    )
    .orderBy(col("ca_total").desc())
)

display(stats_canal)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üí≥ Analyse 5 : R√©partition des Moyens de Paiement

# COMMAND ----------

# R√©partition par moyen de paiement
paiements = (
    sales
    .groupBy("payment")
    .agg(
        count("*").alias("nb_transactions"),
        _sum("revenue").alias("ca_total")
    )
    .withColumn("part_ca", 
                spark_round(col("ca_total") / sales.agg(_sum("revenue")).collect()[0][0] * 100, 1))
    .orderBy(col("ca_total").desc())
)

display(paiements)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5Ô∏è‚É£ Statistiques Descriptives Globales

# COMMAND ----------

# Statistiques sur les colonnes num√©riques
display(sales.select("price", "quantity", "revenue").describe())

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Synth√®se Notebook 1
# MAGIC 
# MAGIC ### Ce que nous avons accompli
# MAGIC 
# MAGIC | √âtape | R√©sultat |
# MAGIC |-------|----------|
# MAGIC | Lecture CSV | 2M lignes charg√©es en ~5 secondes |
# MAGIC | Normalisation | Types corrects + colonne `revenue` |
# MAGIC | Delta Lake | Table persist√©e, optimis√©e, versionn√©e |
# MAGIC | Analyses BI | 5 tableaux/graphiques de pilotage |
# MAGIC 
# MAGIC ### Messages cl√©s
# MAGIC 
# MAGIC 1. **Spark** permet le traitement distribu√© ‚Üí scalabilit√© horizontale
# MAGIC 2. **Delta Lake** = format optimis√© + ACID + time travel
# MAGIC 3. Les agr√©gations sont parall√©lis√©es automatiquement
# MAGIC 
# MAGIC ### Prochaine √©tape
# MAGIC 
# MAGIC üëâ **Notebook 2 : Data Science** ‚Äî Explorer les donn√©es en profondeur et pr√©parer le jeu d'apprentissage ML

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## üìù Exercices Pratiques
# MAGIC 
# MAGIC ### Exercice 1 : CA par Cat√©gorie et Mois
# MAGIC Cr√©ez une analyse croisant `category` et le mois de `order_date`.
# MAGIC 
# MAGIC ### Exercice 2 : Identifier les Pics de Vente
# MAGIC Trouvez les 10 jours avec le plus fort CA. Y a-t-il un pattern (Black Friday, No√´l) ?
# MAGIC 
# MAGIC ### Exercice 3 : Panier Moyen par Pays
# MAGIC Quel pays a le panier moyen le plus √©lev√© ? Le plus bas ?

# COMMAND ----------

# üéØ EXERCICE 1 : Votre code ici
# Indice : utilisez month(col("order_date")) pour extraire le mois

from pyspark.sql.functions import month

# ca_categorie_mois = (
#     sales
#     .withColumn("mois", month(col("order_date")))
#     .groupBy("category", "mois")
#     .agg(_sum("revenue").alias("ca"))
#     .orderBy("category", "mois")
# )
# display(ca_categorie_mois)

# COMMAND ----------

# üéØ EXERCICE 2 : Votre code ici
# Indice : groupBy("order_date"), orderBy desc, limit(10)



# COMMAND ----------

# üéØ EXERCICE 3 : Votre code ici
# Indice : groupBy("country"), agg(avg("revenue"))


